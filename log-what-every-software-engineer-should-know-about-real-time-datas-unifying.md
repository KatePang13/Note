# Log：程序员应当知道的实时数据统一抽象

origin:  [The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

我在 LinkedIn 度过了很愉快的六年时光。那时我们开始遇到单体中心数据库的局限性，有了向分布式系统演进的需要。这是一段精彩的经历：我们构建，部署了多个分布式系统并一直运行到了今天，包括一个分布式图像数据库，一个分布式搜索后端，一个 Hodoop 安装，一个 一代二代 KV 存储。

从这段经历中，我学到的最有用的东西之一是，我们所构建的很多东西的核心都是基于一个非常简单的概念：log。有时我们称之为 write-ahead log 或者 transaction log。 log 几乎伴随这计算机的诞生而诞生，许多分布式数据系统和实时应用架构的核心都是log。

如果你不理解 log，你就不可能完全理解 数据库，NoSQL, KV 存储，复制，PAXOS, hadoop，版本控制，甚至是任何的软件系统；然而，大多数对 log 都不是很熟悉。我希望改变这个现状，在本文中，我将带你逐步了解log相关的所有知识，包括：什么是log；如何使用log进行数据集成，实时处理和系统构建。



## Part 1：什么是Log?

日志或许是最简单的存储抽象。它是 append-only 的，按时间排序的record序列。看起来像这样：

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/log.png)

record被逐一append到日志的结尾，读操作从左到右依次处理。每个 entry 都会分配一个 唯一序列编号 log entry number。

record的顺序定义了时间的概念，左边的entry比右边的entry更早发生。log entry number 可以理解为这个entry的时间戳。将这个顺序描述成时间的概念一开始会有点奇怪，但是它很方便，与任何特定的物理时钟没有之间关联，在分布式系统中，这个特性将变得至关重要。

record 的内容和格式对于此讨论而言并不重要。另外，我们不可能一直向日志中添加记录，因为空间总是会耗尽的。这个后续我会再深入。

所以，日志与文件和表格并没有本质的区别，文件是保存bytes的序列，表格是保存记录的序列，日志其实就是一个文件或者表格，只是记录必须以时序排列。

这里我们可能会疑惑，为什么要讨论一个这么简单的东西。append-only 记录序列是与数据系统有什么关联呢？答案是，日志有一个特定的目的：记录 在什么时候发生了什么（ what happened and when）。对于分布式数据系统，很多时候这都是问题的关键。

在展开讨论之前，我要先澄清一个疑惑。程序员都对另一种日志很熟悉——无结构的错误信息或者应用的追踪信息，使用syslog/log4j 写入到本地文件。为了区分，我将这种日志成为应用程序日志。应用程序日志是我正在描述的日志概念的退化形式。最大的区别在与，文本日志主要是供人类阅读的，而本文讨论的 "journal" 或者 ”data logs“ 是供程序访问的。

（实际上，如果您考虑一下，人类在单台计算机上读取日志的想法已经是不合时宜的了。当涉及到许多服务和服务器时，这种方法很快就变得难以管理，日志的目的迅速转变成了查询和图形的输入，以了解许多机器上的行为——对于这种目的，文件中的文本信息就没有此处描述的那种结构化日志来的合适了）

### Logs in databases

我不知道log概念的起源——或许它就像二分查找那样，看起来过于简单，以至于发明者并没有留意到这是一个发明。它最早出现在IBM的  [System R](http://www.cs.berkeley.edu/~brewer/cs262/SystemR.pdf) 。在数据库中的使用与出现崩溃时保持各种数据结构和索引的同步有关。为了使其具有原子性和持久性，数据库在将更改作用到它数据结构之前，使用日志写出它们将要修改的记录的信息。日志是发生的情况的记录，每个表或索引都是对这个记录的投影。由于日志会立即保留，因此在发生崩溃时，该日志将用作还原所有其他持久性结构的权威来源。

随着时间的流逝，日志的使用从ACID的实现细节发展为在数据库之间复制数据的方法。事实证明，数据库上发生的更改顺序正是保持远程副本数据库同步所需的。 Oracle，MySQL和PostgreSQL包含日志传送协议，以将日志的一部分传输到充当从属的副本数据库。  Oracle已将日志产品化为常规数据订阅机制，提供给非Oracle数据订阅者的 [XStreams](http://docs.oracle.com/cd/E11882_01/server.112/e16545/xstrm_intro.htm)和 [GoldenGate](http://www.oracle.com/technetwork/middleware/goldengate/overview/index.html) 使用，MySQL和PostgreSQL中的类似功能是许多数据体系结构的关键组件。

由于这个起源，机器可读日志的概念在很长一段时间里都局限于数据库内部。使用日志作为数据订阅机制几乎是偶然的。但是，这种非常抽象的方法非常适合支持各种消息传递，数据流和实时数据处理。

### Logs in distributed systems

日志解决的两个问题，序列化变更和分发数据，这在分布式数据系统中显得更为重要。这些分布式系统的核心设计问题之一是对更新顺序达成共识（或者对不一致产生共识并应对这些副作用）。（Agreeing upon an ordering for updates (or agreeing to disagree and coping with the side-effects) are among the core design problems for these systems.）

以日志为中心的分布式系统方法源自一个简单的观察，我将其称为状态机复制原理：

如果2个相同的，确定性的进程，开始于一个相同状态，并且以相同的顺序接收相同的输入，它们将产生相同的输出，并结束于相同的状态。（If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.）

确定性（[Deterministic](http://en.wikipedia.org/wiki/Deterministic_algorithm) ) 表示这个进程是 无时间无关的，不会有任何其他的输入影响其结果。例如，如果一个程序，其输出受线程执行的特定顺序影响，或调用gettimeofday，或某些其他不可重复的东西影响，通常认为是 不确定性的 （non-deterministic）。

进程的状态指的是在处理结束时计算机上保留在内存和磁盘中的任何数据。

**以相同的顺序获得相同的输入**—— 这就是日志的来源。这是一个非常直观的概念：如果对两个确定性代码段输入相同的输入日志，则它们将产生相同的输出。

分布式计算的应用非常明显。您可以将**让多台计算机执行相同操作**，转化成 **实现分布式一致日志以供这些进程输入**。此处日志的目的是从输入流中挤出所有不确定性，以确保处理此输入的每个副本保持同步。

当您理解它时，就会发现这个原理并没有什么复杂或深奥的：它大体的意思就是“**确定性的处理是确定性的**”（**"deterministic processing is deterministic"**）。尽管很简单，我认为它是用于分布式系统设计的通用的工具之一。

这种方法的优点之一是：索引日志的时间戳现在充当了副本状态的时钟——你可以用一个数字来描述每个副本，即已处理的最大日志entry的时间戳。该时间戳与日志相结合，可以独一无二地捕捉这个副本的完整状态。

有多种方法可以根据日志中的内容在系统中应用此原理。例如，我们可以在日志中记录服务请求，或者服务响应于请求而经历的状态更改，或者它执行的转换命令。从理论上讲，我们甚至可以记录一系列机器指令，以便每个副本执行或在每个副本上调用方法名称和参数。只要两个进程以相同的方式处理这些输入，这些进程将在副本之间保持一致。

分布式系统文献通常区分两种主要的处理和复制方法。 “状态机模型”通常是指主动-主动模型（active-active model），其中我们记录传入请求的日志，每个副本处理每个请求。对此的略微修改（称为“主备份模型” ，primary-backup model）是选择一个副本作为领导者，并允许该领导者按请求到达的顺序处理请求，并从处理请求中注销对其状态的更改。领导者进行的状态更改按顺序到应用其他副本，，以便它们同步并准备在领导者失败时接任领导者。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/active_and_passive_arch.png)

要了解这两种方法之间的区别，让我们看一个示例问题。考虑一个复制的“算术服务”，该服务将单个数字作为其状态（初始化为零）并将该值进行加法和乘法。主动-主动方法可能会注销要应用的转换，例如“ +1”，“ * 2”等。每个副本都将应用这些转换，因此要经过相同的一组值。 “主动-被动”方法将使单个主机执行转换并注销结果，例如“ 1”，“ 3”，“ 6”等。此示例还清楚说明了为什么排序对于确保副本之间的一致性至关重要：对加法和乘法进行重新排序将产生不同的结果。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/paxos_postcard.jpg)

分布式日志可以看作是建模共识问题的数据结构。毕竟，日志代表对要附加的“下一个”值的一系列决策。你必须绕以下弯才能看到Paxos系列算法中的日志，尽管日志构建是它们最常见的实际应用。对于Paxos，通常使用协议的扩展名“ multi-paxos”来完成此操作，该协议将日志建模为一系列共识问题，每个问题对应一个日志。日志在[ZAB](http://www.stanford.edu/class/cs347/reading/zab.pdf), [RAFT](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), and [Viewstamped Replication](http://pmg.csail.mit.edu/papers/vr-revisited.pdf) 等其他协议中更为突出，这些协议直接对维护分布式一致日志的问题进行建模。

我的怀疑是，我们对此的看法在一定程度上受到了历史道路的误导，这也许是由于几十年来分布式计算的理论超过了其实际应用。实际上，共识问题有点太简单了。计算机系统很少需要确定单个值，它们几乎总是处理一系列请求。因此，使用日志而不是简单的单值寄存器是更自然的抽象。

此外，对算法的关注掩盖了底层日志抽象系统的需求。我有理由怀疑我们最终将把日志作为商品化的构建模块而不管其实现如何，就像我们经常谈论哈希表一样，而不必费心弄清楚是线性探测还是杂项哈希的细节其他变体。日志将变成商品化的接口，许多算法和实现会相互竞争以提供最佳的保证和最佳的性能。



### Changelog 101: Tables 和 Events 是阴阳两极

让我们回到数据库。变更日志和表格之间有一个令人着迷的对偶。日志类似于所有贷方和借方以及银行流程的列表。表格是所有当前帐户余额。如果有更改日志，则可以应用这些更改以创建捕获当前状态的表。该表将记录每个键的最新状态（以特定的日志时间为准）。从某种意义上说，日志是更基本的数据结构：除了创建原始表之外，您还可以对其进行转换以创建各种派生表。 （是的，表可以表示非关系人员的键控数据存储。）

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/yin-yang.jpg)

反过来，如果您有一个表进行更新，则可以记录这些更改并发布所有更新到该表状态的“更改日志”。此变更日志正是支持近实时副本所需要的。因此，从这个意义上讲，您可以将表和事件看作是互补的：**表支持静态数据，日志捕获更改**。**日志的神奇之处在于，如果它是更改的完整日志，则它不仅保存表的最终版本的内容，而且还允许重新创建可能已经存在的所有其他版本。实际上，它是对表的每个先前状态的一种备份**。

这可能使您想起源代码版本控制。源代码管理与数据库之间有着密切的关系。**版本控制解决了与分布式数据系统必须解决的问题非常相似的问题——管理状态的分布式并发更改**。版本控制系统通常对补丁序列进行建模，实际上就是日志。您可以直接与类似于表的当前代码的已签出“快照”相互作用。您会注意到，在版本控制系统中，就像在其他分布式有状态系统中一样，复制是通过日志进行的：更新时，您仅下拉补丁并将其应用到当前快照。

最近有人从 [Datomic](http://www.datomic.com/)（一家以日志为中心的数据库销售公司）看到了其中一些想法。此演示文稿很好地概述了他们如何在系统中应用该想法。当然，这些想法并不是该系统独有的，因为它们成为分布式系统和数据库文献的一部分已有十多年了。

这看起来似乎有点理论性。不要绝望！我们将很快介绍实用的内容。

### What's next

在本文的其余部分中，我将尝试介绍一种日志所不具备的优点，它超出了分布式计算或抽象分布式计算模型的内部范围。这包括： 

- **数据集成**——使组织的所有数据可以轻松地在其所有存储和处理系统中使用。 
- **实时数据处理**——计算派生数据流。 
- **分布式系统设计**——以日志为中心的设计如何简化实际系统。 

这些都围绕着**将日志作为独立服务的思想**来解决。 在每种情况下，**日志的实用性都来自日志提供的简单功能：生成持久的，可重播的历史记录**。出乎意料的是，这些问题的核心是能够以确定的方式以自己的速率使许多计算机回放历史记录。



## Part Two: 数据集成

首先让我说一下“数据集成”的含义以及为什么我认为它很重要，然后我们将了解它与日志之间的关系。

`数据集成使组织可以在其所有服务和系统中使用所有数据。`

“数据集成”这个词并不常见，但我不知道有什么更好的。术语ETL通常仅涵盖数据集成的一小部分——填充关系数据仓库。但是我所描述的许多内容都可以看作是ETL的泛化，涵盖了实时系统和处理流程。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/cabling.jpg)

数据集成，但是，我相信，“使数据可用”这个平凡的问题是组织可以关注的更有价值的事情之一。

你几乎没有听说过有关数据集成的内容，而更多的是大数据概念的炒作，但是，我认为，“**使数据可用**”这个平凡的问题是大家可以关注的更有价值的事情之一。

有效使用数据遵循Maslow的[需求层次结构](http://en.wikipedia.org/wiki/Maslow's_hierarchy_of_needs)。**金字塔的底部涉及捕获所有相关数据，并能够将其放到适用的处理环境中（例如精美的实时查询系统或仅文本文件和python脚本）。这些数据需要以统一的方式建模，以使其易于读取和处理。一旦满足了以统一方式捕获数据的这些基本需求，就可以在基础架构上以各种方式处理该数据（MapReduce，实时查询系统等）进行合理处理**。

值得注意的是：当时并没有可靠且完整的数据流，Hadoop集群仅是非常昂贵且难以组装的小型供暖炉。一旦数据和处理可用，人们就可以将注意力转移到更好的数据模型和一致的语义理解上。最后，注意力可以转移到更复杂的处理上-更好的可视化，报告以及算法处理和预测。

以我的经验，大多数组织在金字塔的底部都有巨大的漏洞-他们缺乏可靠的完整数据流-但想直接跳到高级数据建模技术。这是完全错误的方向。

所以关键的问题是，**我们应该如何在所有数据系统中建立可靠的数据流？**



#### 数据集成：两种复杂性

有两种趋势正在让数据集成变得愈加困难

**事件数据消防水带**

第一个趋势是事件数据的增长。事件数据记录事情发生而不是事情是什么(. Event data records things that happen rather than things that are. )。在Web系统中，这意味着用户活动日志记录，也意味着可靠地操作和监视数据中心的机器价值所需的机器级别事件和统计信息。人们通常称其为“日志数据”，因为通常将其写入应用程序日志中，但这会使表格与功能混淆。这些数据是现代网络的核心：毕竟，Google的财富是由建立在点击和印象（这些就是事件）的相关管道产生的。

这些东西不仅限于网络公司，只是因为网络公司已经完全数字化，因此更易于使用。财务数据长期以来都是以事件为中心的。  [RFID](http://en.wikipedia.org/wiki/RFID) 将这种跟踪添加到物理对象。我认为这种趋势将随着传统业务和活动的[数字化](http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html)而继续。

这种类型的事件数据记录了发生的事情，并且往往比传统数据库使用的数据大几个数量级。这对加工提出了重大挑战。

**专用数据系统的爆炸式增长**

第二个趋势来自专用数据系统的[爆炸式增长](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.9136)，这些数据系统在过去五年中变得很流行，并且通常免费提供。存在用于[OLAP](https://github.com/metamx/druid/wiki)，[搜索](http://www.elasticsearch.org/)，[简单的在线存储](http://cassandra.apache.org/)，[批处理](http://hadoop.apache.org/)，[图分析](http://graphlab.org/) , [redis](http://spark.incubator.apache.org/) 等的专用系统。

更多种类的更多数据的组合以及将这些数据导入更多系统的愿望导致了巨大的数据集成问题。

### 日志结构的数据流

日志是用于处理系统之间数据流的自然数据结构。配方很简单： **获取组织的所有数据，并将其放入中央日志以进行实时订阅**。

每个逻辑数据源都可以建模为自己的日志。数据源可以是注销事件（例如单击或页面浏览）的应用程序，也可以是接受修改的数据库表。每个订阅系统都将尽快从该日志中读取日志，将每个新记录应用于其自己的存储，并提升其在日志中的位置。订阅者可以是任何类型的数据系统-缓存，Hadoop，另一个站点中的另一个数据库，搜索系统等。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/log_subscription.png)

例如，日志概念为每次更改提供一个逻辑时钟，以此可以测量所有订阅者。这使得关于不同订户系统相对于彼此的状态的推论要简单得多，因为每个订阅者系统都有其已读取的“时间点”。

为了更加具体，请考虑一个简单的情况，假设有一个数据库和一组缓存服务器。日志提供了一种将更新同步到所有这些系统的方法，并提供了有关每个系统各个时间点状态变更的原因。假设我们用日志条目X编写了一条记录，然后需要从缓存中进行读取。如果我们想保证我们不会看到过时的数据，我们只需要确保我们不会从任何尚未复制到X的缓存中读取数据即可。

日志还充当缓冲区，使数据生产与数据消耗之间可以异步进行。由于许多原因，这一点很重要，但特别是当有多个订阅者可能以不同的速率消费时。这意味着，订阅系统可能会崩溃或停机进行维护，并在恢复时赶上来：订阅者以其控制的速度消费。诸如Hadoop或数据仓库之类的批处理系统可能仅每小时或每天消耗一次，而实时查询系统则可能需要实时更新。**原始数据源和日志都不了解各种数据目标系统，因此可以在不更改管道的情况下添加和删除使用者系统**。



![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/tolstoy.jpg)

[“每个工作数据管道的设计都像一个日志；每个断开的数据管道都以自己的方式断开。”-托尔斯泰伯爵（作者翻译）](http://en.wikipedia.org/wiki/Anna_Karenina_principle)

尤其重要的是：目标系统仅知道日志，而不知道源系统的任何详细信息。消费者系统不必担心数据是来自RDBMS，新型的键值存储，还是在没有任何类型的实时查询系统的情况下生成的。这看起来只是一个小问题，但实际上很关键。

我在这里使用术语“ log”代替“ messaging system”或“ pub sub”，因为它在语义上更加具体，并且在实际实现中为支持数据复制所需要的内容更加详尽地描述。我发现“发布订阅”不仅仅意味着间接寻址消息，如果您比较两个承诺发布-订阅的消息系统，您会发现它们可以保证完全不同的事物，并且大多数模型在该领域都没有用。你可以**将日志视为一种具有持久性保证和强大排序语义的消息传递系统**。在分布式系统中，这种通信模型有时会使用[原子广播](http://en.wikipedia.org/wiki/Atomic_broadcast)的名称（名字有点糟糕）。

值得强调的是，日志仍然只是基础架构。这还不是掌握数据流的故事的结局：故事的其余部分围绕**元数据**，**模式**，**兼容性**以及**处理数据结构和演化的所有细节**。但是，除非有可靠，通用的方法来处理数据流的机制，否则语义细节是次要的。

### At LinkedLn

`随着LinkedIn从集中式关系数据库转移到分布式系统集合中，我看到了这种数据集成问题迅速出现。`

这些天，我们的主要数据系统包括：

- [Search](http://data.linkedin.com/projects/search)
- [Social Graph](http://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed)
- [Voldemort](http://project-voldemort.com/) (key-value store)
- [Espresso](http://data.linkedin.com/projects/espresso) (document store)
- [Recommendation engine](http://www.quora.com/LinkedIn-Recommendations/How-does-LinkedIns-recommendation-system-work)
- OLAP query engine
- [Hadoop](http://hadoop.apache.org/)
- [Terradata](http://www.teradata.com/)
- [Ingraphs](http://engineering.linkedin.com/52/autometrics-self-service-metrics-collection) (monitoring graphs and metrics services)

这些都是专业的分布式系统，在其专业领域提供高级功能。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/linkedin.png)

在我来这里之前，就已经在LinkedIn上流传着使用日志进行数据流的想法。我们开发的最早基础架构之一是一项名为[databus](https://github.com/linkedin/databus)的服务，该服务在我们早期的Oracle表之上提供了日志缓存抽象，以扩展对数据库更改的订阅，基于此我们可以提供社交图和搜索索引。

我将提供一些相史料来提供相关的背景。在我们交付键值存储之后，我于2008年左右开始参与于此。我的下一个项目是尝试进行有效的Hadoop设置，并将我们的一些推荐流程移到那里。由于在这方面经验很少，我们自然会预算几个星期来获取和获取数据，而其余时间则用于实现花哨的预测算法。于是开始了一段漫长的艰苦奋斗。

我们最初计划仅将数据从现有的Oracle数据仓库中刮取。第一个发现是，迅速地从Oracle中获取数据是一个黑科技。更糟糕的是，数据仓库处理不适合我们为Hadoop计划的批量生产处理——许多处理是不可逆的，并且特定于正在执行的报告。我们最终了绕开了数据仓库，而直接进入源数据库和日志文件。最后，我们实现了另一个管道，以将数据加载到键值存储中（[load data into our key-value store](http://data.linkedin.com/blog/2009/06/building-a-terabyte-scale-data-cycle-at-linkedin-with-hadoop-and-project-voldemort)）以提供结果。

这种日常的数据复制最终成为开发前期的主要事项。更糟糕的是，每当任何管道出现问题时，Hadoop系统就几乎没法用，对不良数据执行花哨的算法只会产生更多不良数据。

**尽管我们以相当通用的方式构建事物，但是每个新数据源都需要自定义配置才能进行设置**。它也被证明是大量错误和失败的根源。我们在Hadoop上实现的站点功能变得非常流行，并且我们发现了一大堆感兴趣的工程师。**每个用户都有一个他们想要与之集成的系统列表以及一堆他们想要的新数据源**。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/sisyphus.jpg)

我逐渐明白了几件事。

- 第一，尽管有些混乱，但我们建立的管道确实非常有价值。仅仅在新处理系统（Hadoop）中使数据可用的过程就释放了很多可能性。对以前很难做到的数据进行新的计算是可能的。许多新产品和分析只是来自将以前锁定在专用系统中的多个数据组合在一起。 

- 第二，很明显，可靠的数据加载将需要数据管道的深入支持。如果我们捕获了所需的所有结构，就可以使Hadoop数据加载完全自动化，从而无需进行手动操作即可添加新的数据源或处理架构更改——数据会神奇地出现在HDFS中，并且Hive表会自动生成具有相应列的数据源。 

- 第三，我们的数据覆盖率仍然很低。也就是说，如果您查看LinkedIn在Hadoop中可用的数据的总体百分比，那么它仍然是非常不完整的。考虑到操作每个新数据源所需的工作量，要完成工作并非易事。

我们一直在进行的**为每个数据源和目标建立自定义数据负载的方法显然是不可行的**。我们有数十个数据系统和数据存储库。连接所有这些将导致在每对系统之间建立自定义管道，如下所示：

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/datapipeline_complex.png)

请注意，数据经常双向流动，因为许多系统（数据库，Hadoop）都是数据传输的源和目的地。这意味着我们最终将为每个系统建立两个管道：一个用于获取数据，另一个用于获取数据。

显然这将需要大量人员来建设，而且永远无法运作。当我们接近完全连接时，我们最终会得到 O(N^2)  条流水线。

相反，我们需要这样的通用名称：

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/datapipeline_simple.png)

我们需要尽可能地将每个使用者与数据源隔离开。理想情况下，它们应仅与单个数据存储库集成，从而使他们可以访问所有内容。

这样做的想法是，添加新的数据系统（无论是数据源还是数据目标）都应该创建集成工作，只能将其连接到单个管道而不是每个数据使用者。

这种经验使我专注于构建 [Kafka](http://kafka.apache.org/)，以将我们在消息传递系统中看到的内容与在数据库和分布式系统内部流行的日志概念相结合。我们希望**某些东西首先充当所有活动数据的中央管道，并最终用于许多其他用途，包括从Hadoop之外部署数据，监视数据等**。

长期以来，Kafka作为基础架构产品还是有点独特（有些人会说奇怪），既不是数据库，不是日志文件收集系统，也不是传统的消息传递系统。但是最近，亚马逊提供了一种非常类似于Kafka的服务，称为[Kinesis](http://aws.amazon.com/kinesis)。相似之处在于处理分区，保留数据以及在Kafka API中高低级使用者之间相当奇怪的划分。我对此感到非常高兴。创建出良好的基础架构抽象的标志是，AWS提供了它作为服务！他们的愿景似乎与我所描述的完全相似：连接所有分布式系统（DynamoDB，RedShift，S3等）的管道，以及使用EC2进行分布式流处理的基础。

### 与ETL和数据仓库的关系

让我们谈谈数据仓库。数据仓库旨在作为结构清晰的集成数据的存储库，以支持分析。这是一个好主意。对于那些不了解的人，数据仓库方法包括定期从源数据库中提取数据，将其汇总为某种可以理解的形式，然后将其加载到中央数据仓库中。拥有一个包含所有数据的干净副本的中央位置，对于数据密集型分析和处理而言是一笔非常宝贵的资产。从较高的角度来看，无论您使用的是Oracle，Teradata还是Hadoop等传统数据仓库，这种方法都不会改变太多，尽管您可能会改变加载和处理的顺序。

包含干净的集成数据的数据仓库是一项了不起的资产，但是获取这些数据的机制有些过时了。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/oracle.jpg)

以数据为中心的组织方式，存在的主要问题是**将干净的集成数据耦合到数据仓库**。数据仓库是一个批处理查询基础结构，非常适合于多种报告和临时分析，尤其是当查询涉及简单的计数，汇总和过滤时。但是将批处理系统作为唯一的干净完整数据存储库意味着该数据对于需要实时提要的系统（实时处理，搜索索引，监视系统等）是不可用。

在我看来，ETL实际上是两件事。首先，这是一个提取和数据清除过程——实质上**释放了组织中各种系统中锁定的数据，并删除了特定于系统的废话**。其次，**针对数据仓库查询对数据进行重组**（即使其适合关系数据库的类型系统，被强制为星型或雪花模式，可能分解为高性能[列格式](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html)等）。混淆这两件事是有问题的。干净，集成的数据存储库应该实时可用，以便进行低延迟处理以及在其他实时存储系统中建立索引。

更好的方法是使用中央管道（日志）和定义明确的用于添加数据的API。**与该管道集成并提供干净，结构良好的数据提要的责任在于该数据提要的生产者**。这意味着，在系统设计和实现的过程中，他们**必须考虑将数据取出并以结构良好的形式传递到中央管道的问题**。新存储系统的添加对于数据仓库团队来说没有关系，因为它们具有集成的中心点。数据仓库团队仅处理以下较简单的问题：从中央日志加载结构化的数据提要，并执行针对其系统的特定转换。

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/pipeline_ownership.png)

当考虑采用传统数据仓库之外的其他数据系统时，关于**组织可伸缩性**的这一点变得尤为重要。举例来说，有人希望在组织的整个数据集上提供搜索功能。或者说，有人想提供具有实时趋势图和警报的亚秒级数据流监视。在这两种情况下，传统数据仓库甚至Hadoop集群的基础架构都是不合适的。更糟糕的是，为支持数据库负载而构建的ETL处理管道可能无法满足其他系统的需求，这使得引导这些基础设施的工作像采用数据仓库一样大。这不太可行，并且可能有助于解释为什么大多数组织不容易将这些功能用于所有数据。**相比之下，如果组织建立了统一的，结构良好的数据源，则使任何新系统完全访问所有数据仅需要一点点集成管道即可连接到管道**。

对于特定的清理或转换，此体系结构还提出了一组不同的选项：

- 可以在将数据添加到公司范围的日志之前，由数据生产者完成。
- 可以作为日志上的实时转换来完成（这又会生成新的转换后的日志）
- 它可以作为装入某些目标数据系统中的一部分过程来完成

最好的模型是在由数据发布者在发布之前进行清理。这意味着确保数据为规范格式，并且不会保留产生该数据的特定代码或可能已维护该数据的存储系统的任何保留。这些细节最好由创建数据的团队处理，因为他们最了解自己的数据。在此阶段应用的任何逻辑都应该是无损且可逆的。

可以实时进行的任何类型的增值转换，都应作为对原始日志提要的后处理。这将包括事件数据的会话化，或添加其他通常令人感兴趣的字段。原始日志仍然可用，但是这种实时处理会生成包含增强数据的派生日志。

最后，在加载过程中，仅应执行特定于目标系统的聚合。这可能包括将数据转换为特定的星型或雪花模式，以便在数据仓库中进行分析和报告。因为此阶段（最自然地映射到传统ETL流程）现在是在更干净，更统一的流集上完成的，因此应大大简化。

### 日志文件与事件

让我们来谈谈这种体系结构的一个附带好处：支持解耦的，事件驱动的系统。

Web行业中活动数据的典型方法是将记录到文本文件，然后可以将其丢到数据仓库或Hadoop中进行聚合和查询。此问题与所有批处理ETL的问题相同：**它将数据流与数据仓库的功能和处理计划耦合在一起**。

在LinkedIn，我们以日志为中心的方式构建了事件数据处理。我们将Kafka用作中央的多用户事件日志。我们定义了数百种事件类型，每种类型都捕获有关特定类型的操作的独特属性。它涵盖了从页面浏览量，广告展示次数和搜索到服务调用和应用程序异常的所有内容。

要了解此方法的优点，请想象一个简单的事件——在职位页面上显示招聘启事。职位页面应仅包含展示该职位所需的逻辑。但是，在一个相当动态的站点中，很容易将其与与显示工作无关的其他逻辑放在一起。例如，假设我们需要集成以下系统：

1. 我们需要将此数据发送到Hadoop和数据仓库以进行脱机处理 
2. 我们需要计算观看次数，以确保观看者不会尝试抓取某些内容 
3. 我们需要汇总此次访问，以显示在职位发布者的分析页面中 
4. 我们需要记录该视图，以确保适当地限制该用户的各个职位推荐的展示次数（我们不想一遍又一遍地展示相同的内容） 
5. 我们的推荐系统可能需要记录访问以正确跟踪该职位的受欢迎程度 
6. 等等

很快，展示职位的简单动作就变得非常复杂。而且，当我们添加显示的其他位置（移动应用程序等）时，必须保留此逻辑，并且复杂性也会增加。更糟糕的是，我们现在需要与之交互的系统有些交织在一起–显示工作的人员需要了解许多其他系统和功能，并确保它们正确集成。这还仅仅是问题的一个玩具版本，任何实际的应用程序的复杂性只会有过之而无不及。

“事件驱动”模式提供了一种简化方法。职位显示页面现在仅显示一个职位，并记录了已显示与该职位实际相关的各种属性，职位浏览者，以及其他与职位展示相关的其他有用信息。其他的每个感兴趣的系统——推荐系统，安全系统，职位发布者分析系统和数据仓库——都只订阅该信息源并进行处理。职业显示代码不需要知道这些其他系统，并且添加新的数据使用方也不需要更改职位显示代码。

### Building a Scalable Log

当然，将发布者与订阅者分开并不是什么新鲜事。但是，如果您想保留一个提交日志，以充当消费类网站上发生的所有事件的多用户实时日志，则可伸缩性将是主要挑战。如果我们无法构建足够快速，廉价且可扩展的日志以使其大规模实用，那么将日志用作通用集成机制绝不会是一种优雅的幻想。

系统人员通常将分布式日志视为缓慢而繁重的抽象（并且通常仅将其与Zookeeper可能适合的“元数据”使用类型相关联）。但是，如果考虑周到的实现专注于记录大型数据流，则不必如此。在LinkedIn，我们目前每天通过Kafka运行超过600亿个唯一消息写操作（如果从[数据中心之间的镜像](http://kafka.apache.org/documentation.html#datacenters)中计算写操作，则为数千亿个）。

我们在Kafka中使用了一些技巧来支持这种规模： 

1. 对日志做分区(Partitioning ) 
2. 通过批量读写来优化吞吐量 
3. 避免不必要的数据复制 

为了允许水平缩放，我们将日志分成多个分区：

![img](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/partitioned_log.png)

每个分区都是完全有序的日志，但是分区之间没有全局排序（可能您的消息中可能包含一些挂钟时间）。**将消息分配给特定分区的分区策略可由编写者控制，大多数用户选择按某种键（例如，用户ID）进行分区**。分区允许日志追加在分片之间不协调的情况下发生，并且允许系统的吞吐量随Kafka集群大小线性扩展。

**每个分区在可配置数量的副本之间进行复制，每个副本具有相同的分区日志副本**。在任何时候，他们中的一个都会担任领导者；如果领导者失败，其中一个副本将接管领导者。

跨分区缺乏全局顺序是一个限制，但我们觉得这并不重要。**实际上，与日志的交互通常来自成百上千个不同的进程，因此谈论其行为的总顺序是没有意义的。相反，我们提供的保证是每个分区都是按顺序保留的，而Kafka保证从单个发送者追加到特定分区的消息将按照其发送顺序进行分发**。

最后，**Kafka使用一种简单的二进制格式，该格式在内存日志，磁盘日志以及网络数据传输之间维护**。这使我们能够利用多种优化方法，包括[零拷贝数据传输](最后，Kafka使用一种简单的二进制格式，该格式在内存日志，磁盘日志以及网络数据传输之间维护。这使我们能够利用多种优化方法，包括零拷贝数据传输。)。

这些优化的累积效果是，即使维护的数据集大大超出内存，通常也可以以磁盘或网络支持的速率写入和读取数据。

本文并非主要是关于Kafka的，因此我将不进一步讨论。您可以在此处阅读[LinkedIn的方法的更详细概述](http://sites.computer.org/debull/A12june/pipeline.pdf)，并在此处阅读[Kafka的设计的全面概述](http://kafka.apache.org/documentation.html#design) 。



## Part Three: 日志 与 实时流数据处理



## Part Four: 系统构建



## The End

